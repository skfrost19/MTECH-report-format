%%%%%%%%%%%%%%%%%%%%%%% CHAPTER - 1 %%%%%%%%%%%%%%%%%%%%\\

\chapter{Introduction}

\label{C1} %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage


%Cardiovascular diseases are a leading cause of mortality worldwide, and early diagnosis is crucial for effective treatment and management. Recent advancements in natural language processing have led to the development of large language models (LLMs). LLMs have the ability to understand and analyze medical data. LLMs offer fresh, intriguing chances to help physicians diagnose cardiovascular diseases early on.

%\section{What is fine-tuning?}
%Fine-tuning is a process in deep learning where a pre-trained model is taken and its internal weights and biases are adjusted to fit a new, smaller dataset for a specific task. We make use of this approach when a model has already been trained on a large dataset for a general task, and we want to adapt it to a more specific task with limited data. The goal here is to maintain the original properties of the pre-trained model and adapt it for the new task in hand. Fine-tuning makes a model a specialist in the field it is fine-tuned on.
%\section{Why fine-tune?}
%Fine-tuning can help reduce the amount of data and computational resources required to train a model from scratch, which can be particularly useful when working with limited data or computational resources. This has many benefits such as lesser training time and computational resources, and the ability to leverage the knowledge of the pre-trained model. Therefore, it also has lesser impact on the environment than training a new model from scratch.
%\section{What are the different approaches to fine-tuning?}
%\subsection{Feature extraction (repurposing)}
%In this approach, the pre-trained model is used as a feature extractor. The model is frozen, and only the final layer is replaced with a new layer that is trained on the new dataset. This approach is useful when the new dataset is small and similar to the original dataset.
%\subsection{Fine-tuning the entire model}
%In this approach, the entire pre-trained model is fine-tuned on the new dataset. This approach is useful when the new dataset is large and different from the original dataset.

\section{Introduction}
The rapid expansion of textual data across various domains has introduced significant challenges in information retrieval (IR). Conventional methods, such as Term Frequency-Inverse Document Frequency (TF-IDF) and learning-to-rank models, often struggle with understanding complex queries and providing results that are both relevant and contextually meaningful. Moreover, these traditional approaches fail to adequately address challenges such as ambiguous queries and the need for contextual reasoning.

\textit{Retrieval-Augmented Generation (RAG)} represents a transformative approach that combines the robust retrieval capabilities of dense embedding-based search with the generative strengths of large-scale language models (LLMs). By leveraging dense retrieval mechanisms, RAG identifies a subset of relevant documents from a large corpus, which are then utilized by the generative model to produce responses or rank documents effectively. This dual capability enables RAG to excel in open-domain applications, bridging the gap between IR and natural language generation.

\subsection{Retrieval-Augmented Generation (RAG)}
RAG is a hybrid framework that integrates two powerful paradigms: \textit{retrieval} and \textit{generation}. The process involves two primary steps:
\begin{enumerate}
    \item \textbf{Dense Retrieval:} A retriever, often based on neural embeddings, fetches a set of candidate documents or passages relevant to the input query. Dense retrieval mechanisms like DPR (Dense Passage Retrieval) or FAISS provide a scalable and efficient method for retrieving semantically relevant documents.
    \item \textbf{Generative Response:} The retrieved documents are fed into a generative language model, such as GPT-3 or T5, which synthesizes a response or ranks the documents based on contextual relevance to the query.
\end{enumerate}
RAG overcomes the limitations of traditional retrieval systems by leveraging the generative modelâ€™s ability to fill knowledge gaps and reformulate queries dynamically. This approach is particularly effective in scenarios where queries are ambiguous or require contextual understanding beyond surface-level features.

\subsection{Hypothetical Document Embedding (HyDE)}
HyDE extends the capabilities of retrieval systems by generating hypothetical answers to the input query and then evaluating the relevance of documents in relation to the generated answer. The key steps in HyDE are as follows:
\begin{enumerate}
    \item \textbf{Hypothetical Answer Generation:} A generative model hypothesizes an answer to the input query based on prior knowledge and context. This answer is treated as a pseudo-query.
    \item \textbf{Embedding and Comparison:} Both the hypothetical answer and the candidate documents are encoded into dense vector embeddings. The similarity between the embeddings is used to score and rank the documents.
\end{enumerate}
By hypothesizing answers, HyDE effectively bridges the semantic gap between the query and the documents, especially in cases where the original query is incomplete or ambiguous. This technique has been shown to enhance the relevance of ranked documents in complex retrieval tasks.

\subsection{Motivation and Problem Statement}
Ranking documents effectively is a cornerstone of IR systems, especially in knowledge-intensive domains such as academic research, technical documentation, and enterprise search. Traditional ranking methods, such as BM25 or machine learning-based rerankers, rely heavily on surface-level features and fail to capture semantic nuances. On the other hand, the emergence of transformer-based models, such as GPT-3 and BERT, has enabled systems to understand and generate human-like responses, paving the way for novel ranking strategies.

While RAG and HyDE provide robust solutions, further enhancements are required to improve ranking performance, particularly in terms of capturing deeper semantic relationships, reducing noise in retrieval, and optimizing the integration of hypothetical embeddings with generative models.

%\noindent\rule{\linewidth}{2pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\noindent\rule{\linewidth}{2pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
